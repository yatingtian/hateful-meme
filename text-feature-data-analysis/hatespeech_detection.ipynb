{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NpavJ4hZlggE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.11 (default, Jul 27 2021, 07:03:16) \n",
      "[Clang 10.0.0 ]\n",
      "/Users/yatingtian/opt/anaconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmUXIOqNEz4F"
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7vbQRYTKEZRC"
   },
   "outputs": [],
   "source": [
    "def convert_jsonl_pd(filepath):\n",
    "    with open(filepath, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    dat = []\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        dat.append(result)\n",
    "\n",
    "    dat = pd.DataFrame(dat)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Hateful Speech Detection Model\n",
    "\n",
    "From Huggingface\n",
    "- https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain?text=I+like+you.+I+love+you\n",
    "\n",
    "The model is used for classifying a text as Hatespeech, Normal or Offensive. \n",
    "\n",
    "\n",
    "References:\n",
    "- https://huggingface.co/docs/transformers/task_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/Users/yatingtian/nb/omscs/final/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jY1tjvDzHhe6",
    "outputId": "03b475da-377b-4392-8e61-219811eddf9b"
   },
   "outputs": [],
   "source": [
    "# check % of hateful memes with specific word\n",
    "train = convert_jsonl_pd('./vilio/data/train.jsonl')\n",
    "# np.mean(train.label[train.text.apply(lambda x:x.find('asian') != -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70914</td>\n",
       "      <td>img/70914.png</td>\n",
       "      <td>1</td>\n",
       "      <td>tattoos are bad for your health i know 5 milli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>79351</td>\n",
       "      <td>img/79351.png</td>\n",
       "      <td>1</td>\n",
       "      <td>jew mad? get fuhrerious!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25489</td>\n",
       "      <td>img/25489.png</td>\n",
       "      <td>1</td>\n",
       "      <td>brother... a day without a blast is a day wasted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>97180</td>\n",
       "      <td>img/97180.png</td>\n",
       "      <td>1</td>\n",
       "      <td>if there is no race it can't be homicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15872</td>\n",
       "      <td>img/15872.png</td>\n",
       "      <td>1</td>\n",
       "      <td>when you come home from a long day of suicide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id            img  label  \\\n",
       "7   70914  img/70914.png      1   \n",
       "10  79351  img/79351.png      1   \n",
       "12  25489  img/25489.png      1   \n",
       "23  97180  img/97180.png      1   \n",
       "26  15872  img/15872.png      1   \n",
       "\n",
       "                                                 text  \n",
       "7   tattoos are bad for your health i know 5 milli...  \n",
       "10                           jew mad? get fuhrerious!  \n",
       "12   brother... a day without a blast is a day wasted  \n",
       "23           if there is no race it can't be homicide  \n",
       "26  when you come home from a long day of suicide ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.label==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is used for classifying a text as Normal or Abusive (Hatespeech and Offensive).\n",
    "# https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two?text=when+you+finger+an+asian+girl\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "### from models.py\n",
    "from models import *\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
    "model = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
    "inputs = tokenizer(\"He is a great guy\", return_tensors=\"pt\")\n",
    "prediction_logits, _ = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8871, -1.3886]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9636, 0.0364]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(prediction_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [json.loads(jline)['text'] for jline in open('./vilio/data/train.jsonl', \"r\").read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = train_text\n",
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain?text=I+like+you.+I+love+you\n",
    "# The model is used for classifying a text as Hatespeech, Normal or Offensive. \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "inputs = tokenizer(\"He is a great guy\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 7, 7])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['attentions'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04230513423681259, 0.7102062702178955, 0.24748849868774414]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "prediction = torch.softmax(outputs['logits'], dim=1)\n",
    "prediction.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prediction output from bert-base-uncased-hatexplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prediction output from bert-base-uncased-hatexplain\n",
    "def add_hate_proba(text):\n",
    "#     \"\"\"data: Pandas dataframe\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    prediction = torch.softmax(outputs['logits'], dim=1)\n",
    "    prediction = prediction.tolist()[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 'dev_seen'\n",
    "df = convert_jsonl_pd(f'./vilio/data/{part}.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2000/2000 [03:39<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 37s, sys: 2.17 s, total: 3min 39s\n",
      "Wall time: 3min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_output = df.shape[0]\n",
    "df['output'] = df['text'].iloc[:num_output].progress_apply(add_hate_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech_prob</th>\n",
       "      <th>normal_prob</th>\n",
       "      <th>offensive_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026279</td>\n",
       "      <td>0.753008</td>\n",
       "      <td>0.220712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084063</td>\n",
       "      <td>0.629737</td>\n",
       "      <td>0.286199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061047</td>\n",
       "      <td>0.687515</td>\n",
       "      <td>0.251438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080715</td>\n",
       "      <td>0.218909</td>\n",
       "      <td>0.700377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049695</td>\n",
       "      <td>0.678734</td>\n",
       "      <td>0.271571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hatespeech_prob  normal_prob  offensive_prob\n",
       "0         0.026279     0.753008        0.220712\n",
       "1         0.084063     0.629737        0.286199\n",
       "2         0.061047     0.687515        0.251438\n",
       "3         0.080715     0.218909        0.700377\n",
       "4         0.049695     0.678734        0.271571"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df = pd.DataFrame(df['output'].iloc[:num_output].tolist(), columns=['hatespeech_prob', 'normal_prob', 'offensive_prob'])\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([df[:num_output], split_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "      <th>hatespeech_prob</th>\n",
       "      <th>normal_prob</th>\n",
       "      <th>offensive_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15740</td>\n",
       "      <td>img/15740.png</td>\n",
       "      <td>1</td>\n",
       "      <td>when someone tells you how to bbq</td>\n",
       "      <td>[0.026279054582118988, 0.7530084848403931, 0.2...</td>\n",
       "      <td>0.026279</td>\n",
       "      <td>0.753008</td>\n",
       "      <td>0.220712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38794</td>\n",
       "      <td>img/38794.png</td>\n",
       "      <td>1</td>\n",
       "      <td>when they say white folks don't know how to cook</td>\n",
       "      <td>[0.08406342566013336, 0.6297371983528137, 0.28...</td>\n",
       "      <td>0.084063</td>\n",
       "      <td>0.629737</td>\n",
       "      <td>0.286199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60792</td>\n",
       "      <td>img/60792.png</td>\n",
       "      <td>1</td>\n",
       "      <td>the original derp-face</td>\n",
       "      <td>[0.061047375202178955, 0.6875145435333252, 0.2...</td>\n",
       "      <td>0.061047</td>\n",
       "      <td>0.687515</td>\n",
       "      <td>0.251438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71824</td>\n",
       "      <td>img/71824.png</td>\n",
       "      <td>1</td>\n",
       "      <td>okay here you go! you piece of shit!</td>\n",
       "      <td>[0.08071465790271759, 0.21890877187252045, 0.7...</td>\n",
       "      <td>0.080715</td>\n",
       "      <td>0.218909</td>\n",
       "      <td>0.700377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04796</td>\n",
       "      <td>img/04796.png</td>\n",
       "      <td>1</td>\n",
       "      <td>xboxone farming 1619 simulator</td>\n",
       "      <td>[0.04969502240419388, 0.6787340044975281, 0.27...</td>\n",
       "      <td>0.049695</td>\n",
       "      <td>0.678734</td>\n",
       "      <td>0.271571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  15740  img/15740.png      1   \n",
       "1  38794  img/38794.png      1   \n",
       "2  60792  img/60792.png      1   \n",
       "3  71824  img/71824.png      1   \n",
       "4  04796  img/04796.png      1   \n",
       "\n",
       "                                               text  \\\n",
       "0                 when someone tells you how to bbq   \n",
       "1  when they say white folks don't know how to cook   \n",
       "2                            the original derp-face   \n",
       "3              okay here you go! you piece of shit!   \n",
       "4                    xboxone farming 1619 simulator   \n",
       "\n",
       "                                              output  hatespeech_prob  \\\n",
       "0  [0.026279054582118988, 0.7530084848403931, 0.2...         0.026279   \n",
       "1  [0.08406342566013336, 0.6297371983528137, 0.28...         0.084063   \n",
       "2  [0.061047375202178955, 0.6875145435333252, 0.2...         0.061047   \n",
       "3  [0.08071465790271759, 0.21890877187252045, 0.7...         0.080715   \n",
       "4  [0.04969502240419388, 0.6787340044975281, 0.27...         0.049695   \n",
       "\n",
       "   normal_prob  offensive_prob  \n",
       "0     0.753008        0.220712  \n",
       "1     0.629737        0.286199  \n",
       "2     0.687515        0.251438  \n",
       "3     0.218909        0.700377  \n",
       "4     0.678734        0.271571  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "part='test_seen'\n",
    "concat_df = joblib.load(f'input/{part}_concat.joblib')\n",
    "concat_df.to_csv(f'input/{part}_concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete dataframe to save memory space\n",
    "# del split_df\n",
    "# del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check crosstab results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "part='train'\n",
    "concat_df = joblib.load(f'input/{part}_concat.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8596, 8)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "concat_df['max_id'] = concat_df.output.apply(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_text(max_id):\n",
    "    if max_id == 0: return \"Hatespeech\"\n",
    "    elif max_id == 1: return \"Normal\"\n",
    "    else: return \"Offensive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['result'] = concat_df['max_id'].apply(convert_id_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "      <th>hatespeech_prob</th>\n",
       "      <th>normal_prob</th>\n",
       "      <th>offensive_prob</th>\n",
       "      <th>max_id</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>[0.10603009164333344, 0.6264012455940247, 0.26...</td>\n",
       "      <td>0.106030</td>\n",
       "      <td>0.626401</td>\n",
       "      <td>0.267569</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>[0.0399697944521904, 0.6121541857719421, 0.347...</td>\n",
       "      <td>0.039970</td>\n",
       "      <td>0.612154</td>\n",
       "      <td>0.347876</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>[0.04539760947227478, 0.706222653388977, 0.248...</td>\n",
       "      <td>0.045398</td>\n",
       "      <td>0.706223</td>\n",
       "      <td>0.248380</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>[0.13384592533111572, 0.6112882494926453, 0.25...</td>\n",
       "      <td>0.133846</td>\n",
       "      <td>0.611288</td>\n",
       "      <td>0.254866</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>[0.04991312697529793, 0.5152072906494141, 0.43...</td>\n",
       "      <td>0.049913</td>\n",
       "      <td>0.515207</td>\n",
       "      <td>0.434880</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  \\\n",
       "0   its their character not their color that matters   \n",
       "1  don't be afraid to love again everyone is not ...   \n",
       "2                           putting bows on your pet   \n",
       "3  i love everything and everybody! except for sq...   \n",
       "4  everybody loves chocolate chip cookies, even h...   \n",
       "\n",
       "                                              output  hatespeech_prob  \\\n",
       "0  [0.10603009164333344, 0.6264012455940247, 0.26...         0.106030   \n",
       "1  [0.0399697944521904, 0.6121541857719421, 0.347...         0.039970   \n",
       "2  [0.04539760947227478, 0.706222653388977, 0.248...         0.045398   \n",
       "3  [0.13384592533111572, 0.6112882494926453, 0.25...         0.133846   \n",
       "4  [0.04991312697529793, 0.5152072906494141, 0.43...         0.049913   \n",
       "\n",
       "   normal_prob  offensive_prob  max_id  result  \n",
       "0     0.626401        0.267569       1  Normal  \n",
       "1     0.612154        0.347876       1  Normal  \n",
       "2     0.706223        0.248380       1  Normal  \n",
       "3     0.611288        0.254866       1  Normal  \n",
       "4     0.515207        0.434880       1  Normal  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hatespeech-0, Normal-1 or Offensive-2. \n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hatespeech</th>\n",
       "      <td>64</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>4880</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Offensive</th>\n",
       "      <td>637</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label          0     1\n",
       "result                \n",
       "Hatespeech    64   162\n",
       "Normal      4880  1979\n",
       "Offensive    637   874"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "pd.crosstab(concat_df['result'], concat_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6563847429519072\n",
      "0.8743952696649346\n"
     ]
    }
   ],
   "source": [
    "print(1979/(1979+162+874)) #1\n",
    "print(4880/(4880+64+637)) #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hatespeech</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>304</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Offensive</th>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label         0    1\n",
       "result              \n",
       "Hatespeech    3    3\n",
       "Normal      304  171\n",
       "Offensive    33   26"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev_unseen\n",
    "pd.crosstab(concat_df['result'], concat_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hatespeech</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>1118</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Offensive</th>\n",
       "      <td>125</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label          0    1\n",
       "result               \n",
       "Hatespeech     7   16\n",
       "Normal      1118  625\n",
       "Offensive    125  109"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_unseen\n",
    "pd.crosstab(concat_df['result'], concat_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_id = train_concat.id.unique()\n",
    "# print(len(train_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# Read existing feature set\n",
    "# \"id\" in train.jsonl is an integer\n",
    "try:\n",
    "    train_id = [json.loads(jline)['id'] for jline in open(f'./vilio/data/{part}.jsonl', \"r\").read().split('\\n')]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(len(train_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import time\n",
    "import base64\n",
    "# ids=train # train id is int\n",
    "# FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "#               \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "\n",
    "# # vgattr_id = []\n",
    "# with open('./vilio/data/hm_vgattr5050.tsv') as f:\n",
    "#     reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "#     # boxes = args.num_features # Same boxes for all\n",
    "#     for i, item in enumerate(reader):\n",
    "#         # vgattr_id.append(item[\"img_id\"])\n",
    "#         if (int(item[\"img_id\"])) not in ids:\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(item)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument(\"--num_features\", type=int, default=100, help='How many features we have per img (e.g. 100, 80)')\n",
    "# boxes = args.num_features # Same boxes for all\n",
    "# parser.add_argument(\"--topk\", type=int, default=-1, help='For testing only load topk feats from tsv')\n",
    "\n",
    "TOPK=-1\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "              \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "NUM_FEATURES =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tokenized input\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "train_text = [json.loads(jline)['text'] for jline in open(f'./vilio/data/{part}.jsonl', \"r\").read().split('\\n')]\n",
    "batch_sentences = train_text\n",
    "encoded_inputs = tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=2048, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_loc = {}\n",
    "i = 0\n",
    "\"\"\"\n",
    "key (int)\n",
    "value (int)\n",
    "\"\"\"\n",
    "for jline in open(f'./vilio/data/{part}.jsonl', \"r\").read().split('\\n'):\n",
    "    train_id_loc[int(json.loads(jline)['id'])] = i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8291: 0,\n",
       " 46971: 1,\n",
       " 3745: 2,\n",
       " 83745: 3,\n",
       " 80243: 4,\n",
       " 5279: 5,\n",
       " 1796: 6,\n",
       " 53046: 7,\n",
       " 82301: 8,\n",
       " 31752: 9,\n",
       " 27635: 10,\n",
       " 80597: 11,\n",
       " 45368: 12,\n",
       " 17963: 13,\n",
       " 53968: 14,\n",
       " 10749: 15,\n",
       " 25149: 16,\n",
       " 87520: 17,\n",
       " 89071: 18,\n",
       " 9563: 19,\n",
       " 72048: 20,\n",
       " 49826: 21,\n",
       " 26453: 22,\n",
       " 12650: 23,\n",
       " 2568: 24,\n",
       " 83954: 25,\n",
       " 24316: 26,\n",
       " 62035: 27,\n",
       " 65342: 28,\n",
       " 92058: 29,\n",
       " 58672: 30,\n",
       " 54069: 31,\n",
       " 91586: 32,\n",
       " 87130: 33,\n",
       " 95640: 34,\n",
       " 47819: 35,\n",
       " 59140: 36,\n",
       " 13647: 37,\n",
       " 65832: 38,\n",
       " 57621: 39,\n",
       " 40982: 40,\n",
       " 43275: 41,\n",
       " 12973: 42,\n",
       " 74350: 43,\n",
       " 2478: 44,\n",
       " 51607: 45,\n",
       " 65403: 46,\n",
       " 29750: 47,\n",
       " 26547: 48,\n",
       " 2143: 49,\n",
       " 68192: 50,\n",
       " 52603: 51,\n",
       " 5349: 52,\n",
       " 6491: 53,\n",
       " 74013: 54,\n",
       " 27485: 55,\n",
       " 13026: 56,\n",
       " 17265: 57,\n",
       " 91405: 58,\n",
       " 14026: 59,\n",
       " 18945: 60,\n",
       " 83920: 61,\n",
       " 83497: 62,\n",
       " 4569: 63,\n",
       " 39607: 64,\n",
       " 9715: 65,\n",
       " 1925: 66,\n",
       " 94813: 67,\n",
       " 29437: 68,\n",
       " 95038: 69,\n",
       " 64891: 70,\n",
       " 63280: 71,\n",
       " 17682: 72,\n",
       " 84510: 73,\n",
       " 28690: 74,\n",
       " 3567: 75,\n",
       " 95830: 76,\n",
       " 29873: 77,\n",
       " 70953: 78,\n",
       " 56124: 79,\n",
       " 10386: 80,\n",
       " 19530: 81,\n",
       " 94185: 82,\n",
       " 48370: 83,\n",
       " 74058: 84,\n",
       " 45062: 85,\n",
       " 94170: 86,\n",
       " 4538: 87,\n",
       " 83946: 88,\n",
       " 68127: 89,\n",
       " 97453: 90,\n",
       " 39578: 91,\n",
       " 49360: 92,\n",
       " 87034: 93,\n",
       " 6123: 94,\n",
       " 17028: 95,\n",
       " 42903: 96,\n",
       " 71680: 97,\n",
       " 24135: 98,\n",
       " 32875: 99,\n",
       " 79085: 100,\n",
       " 61872: 101,\n",
       " 30579: 102,\n",
       " 62504: 103,\n",
       " 43810: 104,\n",
       " 19730: 105,\n",
       " 37420: 106,\n",
       " 82509: 107,\n",
       " 96284: 108,\n",
       " 97068: 109,\n",
       " 3214: 110,\n",
       " 23810: 111,\n",
       " 91836: 112,\n",
       " 5126: 113,\n",
       " 76092: 114,\n",
       " 64072: 115,\n",
       " 91058: 116,\n",
       " 21643: 117,\n",
       " 19523: 118,\n",
       " 32691: 119,\n",
       " 87169: 120,\n",
       " 92738: 121,\n",
       " 53027: 122,\n",
       " 27195: 123,\n",
       " 43910: 124,\n",
       " 5213: 125,\n",
       " 73945: 126,\n",
       " 56207: 127,\n",
       " 68459: 128,\n",
       " 63175: 129,\n",
       " 82945: 130,\n",
       " 34975: 131,\n",
       " 61973: 132,\n",
       " 16420: 133,\n",
       " 73069: 134,\n",
       " 47056: 135,\n",
       " 56428: 136,\n",
       " 54206: 137,\n",
       " 93172: 138,\n",
       " 56980: 139,\n",
       " 79615: 140,\n",
       " 38047: 141,\n",
       " 94560: 142,\n",
       " 96250: 143,\n",
       " 28951: 144,\n",
       " 27614: 145,\n",
       " 4857: 146,\n",
       " 79603: 147,\n",
       " 26985: 148,\n",
       " 14873: 149,\n",
       " 54893: 150,\n",
       " 48236: 151,\n",
       " 74908: 152,\n",
       " 96472: 153,\n",
       " 89430: 154,\n",
       " 76921: 155,\n",
       " 68409: 156,\n",
       " 16354: 157,\n",
       " 69548: 158,\n",
       " 1456: 159,\n",
       " 17950: 160,\n",
       " 76295: 161,\n",
       " 42580: 162,\n",
       " 96180: 163,\n",
       " 98701: 164,\n",
       " 15243: 165,\n",
       " 41058: 166,\n",
       " 62375: 167,\n",
       " 63507: 168,\n",
       " 31208: 169,\n",
       " 62319: 170,\n",
       " 68253: 171,\n",
       " 34687: 172,\n",
       " 34209: 173,\n",
       " 39827: 174,\n",
       " 46920: 175,\n",
       " 84273: 176,\n",
       " 39018: 177,\n",
       " 54780: 178,\n",
       " 69150: 179,\n",
       " 29843: 180,\n",
       " 19385: 181,\n",
       " 64510: 182,\n",
       " 47103: 183,\n",
       " 61038: 184,\n",
       " 71620: 185,\n",
       " 91602: 186,\n",
       " 30145: 187,\n",
       " 91756: 188,\n",
       " 63745: 189,\n",
       " 47162: 190,\n",
       " 59806: 191,\n",
       " 7135: 192,\n",
       " 35497: 193,\n",
       " 84362: 194,\n",
       " 84015: 195,\n",
       " 87251: 196,\n",
       " 82590: 197,\n",
       " 59738: 198,\n",
       " 35470: 199,\n",
       " 72061: 200,\n",
       " 3524: 201,\n",
       " 86170: 202,\n",
       " 50841: 203,\n",
       " 46087: 204,\n",
       " 42058: 205,\n",
       " 1967: 206,\n",
       " 54930: 207,\n",
       " 38076: 208,\n",
       " 38910: 209,\n",
       " 6723: 210,\n",
       " 2157: 211,\n",
       " 62948: 212,\n",
       " 21075: 213,\n",
       " 73601: 214,\n",
       " 32579: 215,\n",
       " 21567: 216,\n",
       " 1765: 217,\n",
       " 53172: 218,\n",
       " 32568: 219,\n",
       " 1742: 220,\n",
       " 56098: 221,\n",
       " 78612: 222,\n",
       " 34018: 223,\n",
       " 53609: 224,\n",
       " 89362: 225,\n",
       " 80426: 226,\n",
       " 83045: 227,\n",
       " 61503: 228,\n",
       " 41890: 229,\n",
       " 52091: 230,\n",
       " 24396: 231,\n",
       " 72168: 232,\n",
       " 85761: 233,\n",
       " 35719: 234,\n",
       " 90256: 235,\n",
       " 20984: 236,\n",
       " 95086: 237,\n",
       " 49028: 238,\n",
       " 18356: 239,\n",
       " 52634: 240,\n",
       " 80512: 241,\n",
       " 62703: 242,\n",
       " 95176: 243,\n",
       " 27384: 244,\n",
       " 82437: 245,\n",
       " 26943: 246,\n",
       " 9152: 247,\n",
       " 7429: 248,\n",
       " 75639: 249,\n",
       " 46812: 250,\n",
       " 63827: 251,\n",
       " 41276: 252,\n",
       " 56413: 253,\n",
       " 69512: 254,\n",
       " 43175: 255,\n",
       " 54129: 256,\n",
       " 23645: 257,\n",
       " 89425: 258,\n",
       " 16923: 259,\n",
       " 4769: 260,\n",
       " 69815: 261,\n",
       " 91468: 262,\n",
       " 52104: 263,\n",
       " 52031: 264,\n",
       " 8795: 265,\n",
       " 50317: 266,\n",
       " 73914: 267,\n",
       " 3197: 268,\n",
       " 49621: 269,\n",
       " 68530: 270,\n",
       " 98547: 271,\n",
       " 3519: 272,\n",
       " 43698: 273,\n",
       " 84756: 274,\n",
       " 93051: 275,\n",
       " 68257: 276,\n",
       " 12834: 277,\n",
       " 57208: 278,\n",
       " 26439: 279,\n",
       " 53769: 280,\n",
       " 28406: 281,\n",
       " 53418: 282,\n",
       " 10785: 283,\n",
       " 84302: 284,\n",
       " 76015: 285,\n",
       " 34189: 286,\n",
       " 52079: 287,\n",
       " 63987: 288,\n",
       " 73526: 289,\n",
       " 2145: 290,\n",
       " 84102: 291,\n",
       " 32981: 292,\n",
       " 73605: 293,\n",
       " 7382: 294,\n",
       " 64071: 295,\n",
       " 80912: 296,\n",
       " 84762: 297,\n",
       " 39076: 298,\n",
       " 85679: 299,\n",
       " 38095: 300,\n",
       " 52394: 301,\n",
       " 14865: 302,\n",
       " 54819: 303,\n",
       " 46082: 304,\n",
       " 73962: 305,\n",
       " 78462: 306,\n",
       " 49805: 307,\n",
       " 24098: 308,\n",
       " 67435: 309,\n",
       " 60893: 310,\n",
       " 93148: 311,\n",
       " 7351: 312,\n",
       " 16850: 313,\n",
       " 96312: 314,\n",
       " 86195: 315,\n",
       " 84162: 316,\n",
       " 12067: 317,\n",
       " 5938: 318,\n",
       " 43092: 319,\n",
       " 30148: 320,\n",
       " 12045: 321,\n",
       " 92068: 322,\n",
       " 73021: 323,\n",
       " 56241: 324,\n",
       " 28905: 325,\n",
       " 13750: 326,\n",
       " 6582: 327,\n",
       " 75142: 328,\n",
       " 56149: 329,\n",
       " 41796: 330,\n",
       " 31570: 331,\n",
       " 1726: 332,\n",
       " 23785: 333,\n",
       " 56473: 334,\n",
       " 62970: 335,\n",
       " 41035: 336,\n",
       " 10285: 337,\n",
       " 95604: 338,\n",
       " 68401: 339,\n",
       " 50984: 340,\n",
       " 17908: 341,\n",
       " 42631: 342,\n",
       " 61349: 343,\n",
       " 65093: 344,\n",
       " 86357: 345,\n",
       " 74965: 346,\n",
       " 75918: 347,\n",
       " 14975: 348,\n",
       " 92317: 349,\n",
       " 19536: 350,\n",
       " 90236: 351,\n",
       " 30642: 352,\n",
       " 59206: 353,\n",
       " 19075: 354,\n",
       " 79042: 355,\n",
       " 98720: 356,\n",
       " 2634: 357,\n",
       " 84107: 358,\n",
       " 7198: 359,\n",
       " 45702: 360,\n",
       " 46085: 361,\n",
       " 8451: 362,\n",
       " 73482: 363,\n",
       " 28017: 364,\n",
       " 2364: 365,\n",
       " 42538: 366,\n",
       " 47183: 367,\n",
       " 72904: 368,\n",
       " 90243: 369,\n",
       " 41382: 370,\n",
       " 78156: 371,\n",
       " 74250: 372,\n",
       " 97132: 373,\n",
       " 35840: 374,\n",
       " 78134: 375,\n",
       " 93620: 376,\n",
       " 57369: 377,\n",
       " 10362: 378,\n",
       " 36058: 379,\n",
       " 9467: 380,\n",
       " 60183: 381,\n",
       " 47263: 382,\n",
       " 18726: 383,\n",
       " 50379: 384,\n",
       " 6352: 385,\n",
       " 58093: 386,\n",
       " 19508: 387,\n",
       " 2783: 388,\n",
       " 41728: 389,\n",
       " 51306: 390,\n",
       " 50241: 391,\n",
       " 49650: 392,\n",
       " 17045: 393,\n",
       " 89536: 394,\n",
       " 26187: 395,\n",
       " 74906: 396,\n",
       " 28190: 397,\n",
       " 89642: 398,\n",
       " 59170: 399,\n",
       " 30927: 400,\n",
       " 32049: 401,\n",
       " 95173: 402,\n",
       " 50198: 403,\n",
       " 37658: 404,\n",
       " 95613: 405,\n",
       " 43805: 406,\n",
       " 18306: 407,\n",
       " 46518: 408,\n",
       " 18547: 409,\n",
       " 2518: 410,\n",
       " 4621: 411,\n",
       " 98543: 412,\n",
       " 85621: 413,\n",
       " 18367: 414,\n",
       " 84036: 415,\n",
       " 86417: 416,\n",
       " 92567: 417,\n",
       " 63921: 418,\n",
       " 72864: 419,\n",
       " 30586: 420,\n",
       " 83264: 421,\n",
       " 93041: 422,\n",
       " 97305: 423,\n",
       " 3568: 424,\n",
       " 7653: 425,\n",
       " 26397: 426,\n",
       " 3217: 427,\n",
       " 40618: 428,\n",
       " 67208: 429,\n",
       " 3798: 430,\n",
       " 12468: 431,\n",
       " 16842: 432,\n",
       " 68043: 433,\n",
       " 51807: 434,\n",
       " 78914: 435,\n",
       " 47950: 436,\n",
       " 95487: 437,\n",
       " 49260: 438,\n",
       " 20861: 439,\n",
       " 38914: 440,\n",
       " 47016: 441,\n",
       " 14389: 442,\n",
       " 35487: 443,\n",
       " 84150: 444,\n",
       " 13809: 445,\n",
       " 50261: 446,\n",
       " 75286: 447,\n",
       " 64312: 448,\n",
       " 67103: 449,\n",
       " 18059: 450,\n",
       " 90643: 451,\n",
       " 16749: 452,\n",
       " 37814: 453,\n",
       " 92046: 454,\n",
       " 90843: 455,\n",
       " 85237: 456,\n",
       " 98235: 457,\n",
       " 90267: 458,\n",
       " 48296: 459,\n",
       " 29174: 460,\n",
       " 65801: 461,\n",
       " 91763: 462,\n",
       " 94387: 463,\n",
       " 80947: 464,\n",
       " 78251: 465,\n",
       " 94738: 466,\n",
       " 57823: 467,\n",
       " 32415: 468,\n",
       " 5316: 469,\n",
       " 61085: 470,\n",
       " 27498: 471,\n",
       " 37160: 472,\n",
       " 18742: 473,\n",
       " 19243: 474,\n",
       " 54108: 475,\n",
       " 93528: 476,\n",
       " 78659: 477,\n",
       " 67082: 478,\n",
       " 64125: 479,\n",
       " 41296: 480,\n",
       " 53491: 481,\n",
       " 37692: 482,\n",
       " 36201: 483,\n",
       " 29054: 484,\n",
       " 6273: 485,\n",
       " 25061: 486,\n",
       " 20437: 487,\n",
       " 5439: 488,\n",
       " 34528: 489,\n",
       " 93541: 490,\n",
       " 16704: 491,\n",
       " 48792: 492,\n",
       " 7528: 493,\n",
       " 1268: 494,\n",
       " 83675: 495,\n",
       " 37198: 496,\n",
       " 48670: 497,\n",
       " 9863: 498,\n",
       " 97320: 499}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2317,  2111,  ...,     0,     0,     0],\n",
       "        [  101, 16534,  2012,  ...,     0,     0,     0],\n",
       "        [  101,  2115,  2344,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2298,  2012,  ...,     0,     0,     0],\n",
       "        [  101,  7578,  2177,  ...,     0,     0,     0],\n",
       "        [  101,  1000,  2043,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj_tsv(fname, ids, encoded_inputs, id_loc, concat_df, topk):\n",
    "    \"\"\"Load object features from tsv file.\n",
    "    :param fname: The path to the tsv file.\n",
    "    :param topk: Only load features for top K images (lines) in the tsv file.\n",
    "        Will load all the features if topk is either -1 or None.\n",
    "    :return: A list of image object features where each feature is a dict.\n",
    "        See FILENAMES above for the keys in the feature dict.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    start_time = time.time()\n",
    "    print(\"Start to load Faster-RCNN detected objects from %s\" % fname)\n",
    "    with open(fname) as f:\n",
    "        reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "        boxes = NUM_FEATURES # Same boxes for all\n",
    "\n",
    "        for i, item in enumerate(tqdm(reader)):\n",
    "            \n",
    "            # Check if id in list of ids to save memory\n",
    "            if int(item[\"img_id\"]) not in ids:#modified\n",
    "                continue\n",
    "\n",
    "            for key in ['img_h', 'img_w', 'num_boxes']:\n",
    "                item[key] = int(item[key])\n",
    "            \n",
    "            boxes = item['num_boxes']\n",
    "            decode_config = [\n",
    "                ('objects_id', (boxes, ), np.int64),\n",
    "                ('objects_conf', (boxes, ), np.float32),\n",
    "                ('attrs_id', (boxes, ), np.int64),\n",
    "                ('attrs_conf', (boxes, ), np.float32),\n",
    "                ('boxes', (boxes, 4), np.float32),\n",
    "                ('features', (boxes, -1), np.float32),\n",
    "            ]\n",
    "            for key, shape, dtype in decode_config:\n",
    "                item[key] = np.frombuffer(base64.b64decode(item[key]), dtype=dtype)\n",
    "                try:\n",
    "                    item[key] = item[key].reshape(shape)\n",
    "                except:\n",
    "                    # In 1 out of 10K cases, the shape comes out wrong; We make necessary adjustments\n",
    "                    shape = list(shape)\n",
    "                    shape[0] += 1\n",
    "                    shape = tuple(shape)\n",
    "                    item[key] = item[key].reshape(shape)  \n",
    " \n",
    "                item[key].setflags(write=False)\n",
    "            \n",
    "            # Add tokenized inputs\n",
    "            loc = id_loc[int(item[\"img_id\"])]\n",
    "            \n",
    "            item['features'] = np.vstack([item['features'], encoded_inputs['input_ids'][loc].reshape(1,-1)])\n",
    "            \n",
    "            # Add probability\n",
    "            hatespeech_prob = concat_df[concat_df.id == int(item[\"img_id\"])]['hatespeech_prob'].to_numpy()\n",
    "            hatespeech_prob = np.pad(hatespeech_prob, (2047, 0), mode='constant')\n",
    "            \n",
    "            normal_prob = concat_df[concat_df.id == int(item[\"img_id\"])]['normal_prob'].to_numpy()\n",
    "            normal_prob = np.pad(normal_prob, (2047, 0), mode='constant')\n",
    "            \n",
    "            offensive_prob = concat_df[concat_df.id == int(item[\"img_id\"])]['offensive_prob'].to_numpy()\n",
    "            offensive_prob = np.pad(offensive_prob, (2047, 0), mode='constant')\n",
    "            \n",
    "            item['features'] = np.vstack([item['features'], hatespeech_prob.reshape(1,-1)])\n",
    "            item['features'] = np.vstack([item['features'], normal_prob.reshape(1,-1)])\n",
    "            item['features'] = np.vstack([item['features'], offensive_prob.reshape(1,-1)])\n",
    "            \n",
    "            data.append(item)\n",
    "            if topk is not None and len(data) == topk:\n",
    "                break\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Loaded %d images in file %s in %d seconds.\" % (len(data), fname, elapsed_time))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load Faster-RCNN detected objects from ./vilio/data/hm_vgattr5050.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12140it [01:22, 147.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 images in file ./vilio/data/hm_vgattr5050.tsv in 82 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fname = './vilio/data/hm_vgattr5050.tsv'\n",
    "concat_df = joblib.load(f'input/{part}_concat.joblib')\n",
    "data_out = load_obj_tsv(fname, train_id, encoded_inputs, train_id_loc, concat_df, TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   int64\n",
      "img                 object\n",
      "label                int64\n",
      "text                object\n",
      "output              object\n",
      "hatespeech_prob    float64\n",
      "normal_prob        float64\n",
      "offensive_prob     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "concat_df['id']=concat_df['id'].astype('int')\n",
    "print(concat_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 2048)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out[0]['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.1])\n",
    "np.pad(a, (2047,0), mode='constant').reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_dataset' from 'datasets' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wr/00zlxjb952x4y0j3_hwt4q8r0000gn/T/ipykernel_79279/3952815588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from 'datasets' (unknown location)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771ea6336d3f4f32bc0e7bde63d139a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27222fc8d1b48ddacc8bdc2c6d8c698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e748dcdd978448228327ad2bb82ac6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a315a38c684ca7b68c87ce04032666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "keyword_object_tag.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vilio",
   "language": "python",
   "name": "vilio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
