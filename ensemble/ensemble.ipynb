{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groud truth of validation set and testing seting set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5481\n",
       "1    3019\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('../../data/hateful_memes/train.jsonl', lines=True)\n",
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(540, 4), (500, 4), (2000, 4)]\n"
     ]
    }
   ],
   "source": [
    "dev_unseen = pd.read_json('../../data/hateful_memes/dev_unseen.jsonl', lines=True)\n",
    "dev_seen = pd.read_json('../../data/hateful_memes/dev_seen.jsonl', lines=True)\n",
    "\n",
    "test_unseen = pd.read_json('../../data/hateful_memes/test_unseen.jsonl', lines=True)\n",
    "test_seen = pd.read_json('../../data/hateful_memes/test_seen.jsonl', lines=True)\n",
    "\n",
    "print([dev_unseen.shape, dev_seen.shape, test_unseen.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and validation data for Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Tag\n",
    "Find negative tags in the text of the memes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 12)\n",
      "(640, 12)\n",
      "(640, 2)\n"
     ]
    }
   ],
   "source": [
    "dev_unseen_tag = pd.read_csv('../keyword_search/dev_unseen.csv')\n",
    "dev_seen_tag = pd.read_csv('../keyword_search/dev_seen.csv') \n",
    "test_unseen_tag = pd.read_csv('../keyword_search/test_unseen.csv')\n",
    "\n",
    "dev_full_tag = dev_seen_tag.append(dev_unseen_tag)\n",
    "print(dev_full_tag.shape)\n",
    "\n",
    "# remove duplicates\n",
    "dev_full_tag = dev_full_tag.drop_duplicates(['id'])\n",
    "print(dev_full_tag.shape) \n",
    "\n",
    "dev_full_tag.loc[:, 'profanity':'religion'] = np.where(dev_full_tag.loc[:, 'profanity':'religion'].isnull(), 0, 1)\n",
    "#print(dev_full_tag.head())\n",
    "\n",
    "test_unseen_tag.loc[:, 'profanity':'religion'] = np.where(test_unseen_tag.loc[:, 'profanity':'religion'].isnull(), 0, 1)\n",
    "# hateful speech\n",
    "dev_unseen_htspch = pd.read_csv('../model-result/dev_unseen_concat.csv')\n",
    "dev_seen_htspch = pd.read_csv('../model-result/dev_seen_concat.csv')\n",
    "test_unseen_htsptch = pd.read_csv('../model-result/test_unseen_concat.csv')\n",
    "#print(dev_unseen_htspch.head())\n",
    "dev_full_htspch = dev_seen_htspch.append(dev_unseen_htspch).drop_duplicates(['id']).loc[:, ['id', 'hatespeech_prob']]\n",
    "print(dev_full_htspch.shape)\n",
    "\n",
    "test_unseen_htsptch = test_unseen_htsptch.loc[:, ['id', 'hatespeech_prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction results from diffferent model\n",
    "Combine prediction with the actual data, joining by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(540, 26), (500, 26), (2000, 26)]\n"
     ]
    }
   ],
   "source": [
    "models = ['vilbert', 'visualbert', 'visualbert_coco', 'detectron_VisualBert_cc_10pct', \n",
    "'detectron_VisualBert_cc_50pct', 'detectron_VisualBert_coco', 'detectron_VisualBert_coco_RoBERTa', 'detectron_VisualBert_coco_focalLoss','detectron_VisualBert_cc_50pct_focalLoss','uniter_36feats', 'uniter_50feats']\n",
    "\n",
    "dev_seen_all = dev_seen.copy()\n",
    "dev_unseen_all = dev_unseen.copy()\n",
    "\n",
    "test_seen_all = test_seen.copy()\n",
    "test_unseen_all = test_unseen.copy()\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    # dev_unseen\n",
    "    pred_dev_unseen = pd.read_csv('../model-result/'+model+'_dev_unseen'+'.csv')\n",
    "    if(model in ['uniter_36feats', 'uniter_50feats']):\n",
    "       pred_dev_unseen['proba']=np.exp(pred_dev_unseen['proba']) / (1 +np.exp(pred_dev_unseen['proba']))\n",
    "    pred_dev_unseen = pred_dev_unseen.rename(columns={'proba':model+'_proba', 'label':model+'_label'})\n",
    "    dev_unseen_all = dev_unseen_all.merge(pred_dev_unseen, on = 'id', how = 'inner')\n",
    "   \n",
    "    # dev_seen\n",
    "    pred_dev_seen = pd.read_csv('../model-result/'+model+'_dev_seen'+'.csv')\n",
    "    if(model in ['uniter_36feats', 'uniter_50feats']):\n",
    "       pred_dev_seen['proba']=np.exp(pred_dev_seen['proba']) / (1 +np.exp(pred_dev_seen['proba']))\n",
    "    pred_dev_seen = pred_dev_seen.rename(columns={'proba':model+'_proba', 'label':model+'_label'})\n",
    "    dev_seen_all = dev_seen_all.merge(pred_dev_seen, on = 'id', how = 'inner') \n",
    "    #print(pred_dev_seen.shape)\n",
    "\n",
    "    # test_unseen\n",
    "    pred_test_unseen = pd.read_csv('../model-result/'+model+'_test_unseen'+'.csv')\n",
    "    if(model in ['uniter_36feats', 'uniter_50feats']):\n",
    "       pred_test_unseen['proba']=np.exp(pred_test_unseen['proba']) / (1 +np.exp(pred_test_unseen['proba']))\n",
    "    pred_test_unseen = pred_test_unseen.rename(columns={'proba':model+'_proba', 'label':model+'_label'})\n",
    "    test_unseen_all = test_unseen_all.merge(pred_test_unseen, on = 'id', how = 'inner')\n",
    "\n",
    "print([dev_unseen_all.shape,dev_seen_all.shape, test_unseen_all.shape])\n",
    "    # test_seen\n",
    "    # pred_test_seen = pd.read_csv('../model-result/'+model+'_test_seen'+'.csv')\n",
    "    # pred_test_seen = pred_test_seen.rename(columns={'proba':model+'_proba', 'label':model+'_label'})\n",
    "    # test_seen_all = test_seen_all.merge(pred_test_seen, on = 'id', how = 'inner')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function report accuracy, f1_score, auc w.r.t model\n",
    "def model_metrics(df, model_name):\n",
    "    \n",
    "    model_all = []\n",
    "    acc_all = []\n",
    "    f1_all = []\n",
    "    auc_all = []\n",
    "    for model in model_name:\n",
    "        model_all.append(model)\n",
    "        model_label = model + '_label'\n",
    "        model_prob = model + '_proba'\n",
    "        acc_all.append(metrics.accuracy_score(df.loc[:, 'label'], df.loc[:, model_label]))\n",
    "        f1_all.append(metrics.f1_score(df.loc[:,'label'], df.loc[:, model_label]))\n",
    "        fpr, tpr, _ = metrics.roc_curve(df.loc[:,'label'], df.loc[:, model_prob])\n",
    "        auc_all.append(metrics.auc(fpr, tpr))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Model'] = model_all\n",
    "    df['acc'] = acc_all\n",
    "    df['f1'] = f1_all\n",
    "    df['auc'] = auc_all\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_seen_summary = model_metrics(dev_seen_all, models)\n",
    "dev_seen_summary.to_csv('../model-result/dev_seen_summary'+'.csv')\n",
    "\n",
    "dev_unseen_summary = model_metrics(dev_unseen_all, models)\n",
    "dev_unseen_summary.to_csv('../model-result/dev_unseen_summary'+'.csv')\n",
    "\n",
    "# test_seen_summary = model_metrics(test_seen_all, models)\n",
    "# test_seen_summary.to_csv('../model-result/test_seen_summary'+'.csv')\n",
    "\n",
    "test_unseen_summary = model_metrics(test_unseen_all, models)\n",
    "test_unseen_summary.to_csv('../model-result/test_unseen_summary'+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set for Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'img', 'label', 'text', 'profanity', 'nationality',\n",
       "       'racism', 'gender', 'disability', 'pregnancy', 'religion',\n",
       "       'vilbert_proba', 'vilbert_label', 'visualbert_proba',\n",
       "       'visualbert_label', 'visualbert_coco_proba', 'visualbert_coco_label',\n",
       "       'detectron_VisualBert_cc_10pct_proba',\n",
       "       'detectron_VisualBert_cc_10pct_label',\n",
       "       'detectron_VisualBert_cc_50pct_proba',\n",
       "       'detectron_VisualBert_cc_50pct_label',\n",
       "       'detectron_VisualBert_coco_proba', 'detectron_VisualBert_coco_label',\n",
       "       'detectron_VisualBert_coco_RoBERTa_proba',\n",
       "       'detectron_VisualBert_coco_RoBERTa_label',\n",
       "       'detectron_VisualBert_coco_focalLoss_proba',\n",
       "       'detectron_VisualBert_coco_focalLoss_label',\n",
       "       'detectron_VisualBert_cc_50pct_focalLoss_proba',\n",
       "       'detectron_VisualBert_cc_50pct_focalLoss_label', 'uniter_36feats_proba',\n",
       "       'uniter_36feats_label', 'uniter_50feats_proba', 'uniter_50feats_label',\n",
       "       'hatespeech_prob'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine dev_unseen prediction\n",
    "pred_dev_all = dev_seen_all.append(dev_unseen_all).drop_duplicates('id')\n",
    "# print(pred_dev_all.shape)\n",
    "pred_dev_all = pred_dev_all.drop(['img', 'label', 'text'], axis = 1)\n",
    "#print(pred_dev_all.head())\n",
    "\n",
    "# join pred_dev with the tags\n",
    "dev_full = pd.merge(dev_full_tag, pred_dev_all, on = 'id')\n",
    "dev_full = pd.merge(dev_full, dev_full_htspch, on = 'id')\n",
    "dev_full.to_csv('../model-result/ensemble_train.csv')\n",
    "dev_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set for Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'profanity', 'nationality', 'racism', 'gender',\n",
       "       'disability', 'pregnancy', 'religion', 'img', 'label', 'text',\n",
       "       'vilbert_proba', 'vilbert_label', 'visualbert_proba',\n",
       "       'visualbert_label', 'visualbert_coco_proba', 'visualbert_coco_label',\n",
       "       'detectron_VisualBert_cc_10pct_proba',\n",
       "       'detectron_VisualBert_cc_10pct_label',\n",
       "       'detectron_VisualBert_cc_50pct_proba',\n",
       "       'detectron_VisualBert_cc_50pct_label',\n",
       "       'detectron_VisualBert_coco_proba', 'detectron_VisualBert_coco_label',\n",
       "       'detectron_VisualBert_coco_RoBERTa_proba',\n",
       "       'detectron_VisualBert_coco_RoBERTa_label',\n",
       "       'detectron_VisualBert_coco_focalLoss_proba',\n",
       "       'detectron_VisualBert_coco_focalLoss_label',\n",
       "       'detectron_VisualBert_cc_50pct_focalLoss_proba',\n",
       "       'detectron_VisualBert_cc_50pct_focalLoss_label', 'uniter_36feats_proba',\n",
       "       'uniter_36feats_label', 'uniter_50feats_proba', 'uniter_50feats_label',\n",
       "       'hatespeech_prob'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine dev_unseen prediction\n",
    "pred_test_unseen_all = test_unseen_all.drop(['img', 'label', 'text'], axis = 1)\n",
    "\n",
    "# join pred_dev with the tags\n",
    "test_unseen_tag = test_unseen_tag.drop(['img', 'label','text'], axis = 1)\n",
    "test_full = pd.merge(test_unseen, pred_test_unseen_all, on = 'id')\n",
    "test_full = pd.merge(test_unseen_tag, test_full, on = 'id')\n",
    "test_full = pd.merge(test_full, test_unseen_htsptch,on = 'id')\n",
    "test_full.to_csv('../model-result/ensemble_test.csv')\n",
    "test_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "## Helper functions\n",
    "Functions: calcualte metrics: accuary and auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = ['visualbert_coco', 'detectron_VisualBert_cc_10pct', 'detectron_VisualBert_cc_50pct', 'detectron_VisualBert_coco','detectron_VisualBert_coco_focalLoss','detectron_VisualBert_cc_50pct_focalLoss','uniter_36feats', 'uniter_50feats']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Voting\n",
    "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only reading the labels from different models. The final label is the majority labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>major_vote</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.757897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model       acc        f1       auc\n",
       "0  major_vote  0.731481  0.545455  0.757897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_vote(df, model_name):\n",
    "    np.random.seed(1)\n",
    "    model_label = [s + '_label' for s in model_name]\n",
    "    \n",
    "    major_vote = np.array(df[model_label].median(axis=1).values)\n",
    "    # for major half vs half, random choose one\n",
    "    equal_idx = np.where(major_vote==0.5)[0]\n",
    "    \n",
    "    equal_idx_label = np.random.choice([0,1], size = equal_idx.size)\n",
    "    major_vote[equal_idx] = equal_idx_label\n",
    "    \n",
    "    df['major_vote_label'] = major_vote\n",
    "    major_vote_proba = np.zeros(df.shape[0])\n",
    "    model_name = np.array(model_name)\n",
    "    for i in np.arange(major_vote.size):\n",
    "        row_i_idx = np.where(df.loc[i][model_label] == major_vote[i])[0]\n",
    "        model_i = model_name[row_i_idx]\n",
    "        model_proba_i = [s + '_proba' for s in model_i]\n",
    "        major_vote_proba[i] = np.mean(df.loc[i][model_proba_i].values)\n",
    "    df['major_vote_proba'] = major_vote_proba \n",
    "    \n",
    "    return(model_metrics(df, ['major_vote']))\n",
    "# val\n",
    "# max_vote(dev_full, models)\n",
    "max_vote(dev_unseen_all, ensemble_model)\n",
    "# test\n",
    "\n",
    "#max_vote(pred_test_tmp, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average\n",
    "Proba is calcuated by averaging all the model probability. New label = 1 if proba is larger than 0.5, o.w label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average_vote</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.778397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model       acc        f1       auc\n",
       "0  average_vote  0.718519  0.486486  0.778397"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_vote(df, model_name):\n",
    "   \n",
    "    model_proba = [s + '_proba' for s in model_name]\n",
    "    \n",
    "    average_vote_proba = np.array(df[model_proba].mean(axis=1).values)\n",
    "    average_vote_label = [1 if i > 0.5 else 0 for i in average_vote_proba]\n",
    "    df['average_vote_proba'] = average_vote_proba \n",
    "    df['average_vote_label'] = average_vote_label\n",
    "    return(model_metrics(df, ['average_vote']))\n",
    "    \n",
    "# val\n",
    "average_vote(dev_unseen_all, ensemble_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# plot\n",
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 10)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../model-result/ensemble_train.csv')\n",
    "test_data = pd.read_csv('../model-result/ensemble_test.csv')\n",
    "vars = ['profanity', 'nationality', 'racism', 'gender', 'disability', 'pregnancy', 'religion',  'uniter_36feats_proba', 'uniter_50feats_proba','hatespeech_prob']\n",
    "extra_var = ['profanity', 'nationality', 'racism', 'gender', 'disability', 'pregnancy', 'religion',  'uniter_36feats_proba', 'uniter_50feats_proba', 'hatespeech_prob', 'detectron_VisualBert_cc_10pct_proba',\n",
    "       'detectron_VisualBert_cc_50pct_proba',\n",
    "       'detectron_VisualBert_coco_proba',\n",
    "       'detectron_VisualBert_coco_focalLoss_proba',\n",
    "       'detectron_VisualBert_cc_50pct_focalLoss_proba'] \n",
    "X_train = train_data.loc[:, vars]\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = test_data.loc[:, vars]\n",
    "y_test = test_data['label']\n",
    "print(X_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_data, test_data, vars, param_list, random_state, file_path, n_iter):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    \n",
    "    X_train = train_data.loc[:, vars]\n",
    "    y_train = train_data['label']\n",
    "\n",
    "    X_test = test_data.loc[:, vars]\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_list,n_iter = n_iter, cv = 3, verbose=2, random_state=random_state, n_jobs = -1)\n",
    "\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    print(rf_random.best_estimator_)\n",
    "\n",
    "    # save \n",
    "    pickle.dump(rf_random, open(file_path +'/rf'+str(random_state)+'.pkl', 'wb'))\n",
    "    with open(file_path +'/rf'+str(random_state)+'.txt', 'w') as f:\n",
    "        f.write(str(rf_random.best_params_))\n",
    "        \n",
    "    # prediction\n",
    "    y_pred = rf_random.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred_label = rf_random.best_estimator_.predict(X_test) \n",
    "    acc = metrics.accuracy_score(y_test, y_pred_label)\n",
    "    f1 = metrics.f1_score(y_test, y_pred_label)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    measure_tab = pd.DataFrame([{'Acc': acc,'F1': f1, 'auc': auc}])\n",
    "    measure_tab.to_csv(file_path +'/rf'+str(random_state)+'.csv',index = False)\n",
    "    return measure_tab, rf_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling -----------------\n",
    "# Number of trees in random forest\n",
    "n_estimators = sp_randint(2, 50)\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = sp_randint(2, 20)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = sp_randint(2, 10)\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf =sp_randint(1, 5)\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature =  ['profanity', 'nationality', 'racism', 'gender', 'disability', 'pregnancy', 'religion' , 'hatespeech_prob']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models\n",
    "['visualbert_coco','detectron_VisualBert_cc_10pct', 'detectron_VisualBert_cc_50pct', 'detectron_VisualBert_coco','detectron_VisualBert_coco_focalLoss','detectron_VisualBert_cc_50pct_focalLoss','detectron_VisualBert_coco_RoBERTa','uniter_36feats', 'uniter_50feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profanity',\n",
       " 'nationality',\n",
       " 'racism',\n",
       " 'gender',\n",
       " 'disability',\n",
       " 'pregnancy',\n",
       " 'religion',\n",
       " 'hatespeech_prob',\n",
       " 'visualbert_coco',\n",
       " 'detectron_VisualBert_cc_10pct',\n",
       " 'detectron_VisualBert_cc_50pct',\n",
       " 'detectron_VisualBert_coco',\n",
       " 'detectron_VisualBert_coco_focalLoss',\n",
       " 'detectron_VisualBert_cc_50pct_focalLoss',\n",
       " 'detectron_VisualBert_coco_RoBERTa',\n",
       " 'uniter_36feats',\n",
       " 'uniter_50feats']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n",
      "RandomForestClassifier(bootstrap=False, max_depth=3, max_features='sqrt',\n",
      "                       min_samples_leaf=4, min_samples_split=5,\n",
      "                       n_estimators=12)\n",
      "test_unseen\n",
      "        Model     acc        f1       auc\n",
      "0  major_vote  0.7485  0.594682  0.788378\n",
      "          Model     acc        f1      auc\n",
      "0  average_vote  0.7395  0.555081  0.80313\n",
      "dev_unseen\n",
      "        Model       acc        f1      auc\n",
      "0  major_vote  0.738889  0.557994  0.75775\n",
      "          Model       acc        f1       auc\n",
      "0  average_vote  0.716667  0.488294  0.780882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "version = 'all_vars2'\n",
    "n_iter = 1000\n",
    "file_path = '../model-result/randomForest/model'+str(version)\n",
    "\n",
    "all_models = ['visualbert_coco','detectron_VisualBert_cc_10pct', 'detectron_VisualBert_cc_50pct', 'detectron_VisualBert_coco','detectron_VisualBert_coco_focalLoss','detectron_VisualBert_cc_50pct_focalLoss','detectron_VisualBert_coco_RoBERTa','uniter_36feats', 'uniter_50feats']\n",
    "all_vars = text_feature + [s + '_proba' for s in all_models]\n",
    "measure_tab, rf_random = main(train_data, test_data, all_vars, param_list=random_grid, file_path = file_path, random_state = 17, n_iter = n_iter)  \n",
    "print('test_unseen')\n",
    "print(max_vote(test_data, all_models))\n",
    "print(average_vote(test_data, all_models))\n",
    "\n",
    "print('dev_unseen')\n",
    "print(max_vote(dev_unseen_all, all_models))\n",
    "print(average_vote(dev_unseen_all, all_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualVert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n",
      "RandomForestClassifier(max_depth=2, max_features='sqrt', min_samples_leaf=3,\n",
      "                       min_samples_split=8, n_estimators=30)\n",
      "test_unseen\n",
      "        Model     acc        f1       auc\n",
      "0  major_vote  0.7435  0.585287  0.782558\n",
      "          Model    acc        f1       auc\n",
      "0  average_vote  0.746  0.588331  0.790391\n",
      "dev_unseen\n",
      "        Model       acc        f1       auc\n",
      "0  major_vote  0.724074  0.544343  0.752015\n",
      "          Model       acc      f1       auc\n",
      "0  average_vote  0.724074  0.5387  0.770426\n"
     ]
    }
   ],
   "source": [
    "version = 'visual_bert'\n",
    "n_iter = 1000\n",
    "file_path = '../model-result/randomForest/model_'+str(version)\n",
    "\n",
    "all_models = ['visualbert_coco','detectron_VisualBert_cc_10pct', 'detectron_VisualBert_cc_50pct', 'detectron_VisualBert_coco','detectron_VisualBert_coco_focalLoss','detectron_VisualBert_cc_50pct_focalLoss','detectron_VisualBert_coco_RoBERTa']\n",
    "all_vars = text_feature + [s + '_proba' for s in all_models]\n",
    "measure_tab, rf_random = main(train_data, test_data, all_vars, param_list=random_grid, file_path = file_path, random_state = 17, n_iter = n_iter)  \n",
    "print('test_unseen')\n",
    "print(max_vote(test_data, all_models))\n",
    "print(average_vote(test_data, all_models))\n",
    "\n",
    "print('dev_unseen')\n",
    "print(max_vote(dev_unseen_all, all_models))\n",
    "print(average_vote(dev_unseen_all, all_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n",
      "RandomForestClassifier(bootstrap=False, max_depth=2, max_features='log2',\n",
      "                       min_samples_leaf=2, min_samples_split=4,\n",
      "                       n_estimators=26)\n",
      "test_unseen\n",
      "        Model    acc        f1       auc\n",
      "0  major_vote  0.749  0.631424  0.795156\n",
      "          Model    acc   f1       auc\n",
      "0  average_vote  0.625  0.0  0.802213\n",
      "dev_unseen\n",
      "        Model       acc       f1       auc\n",
      "0  major_vote  0.725926  0.54878  0.810588\n",
      "          Model      acc   f1       auc\n",
      "0  average_vote  0.62963  0.0  0.811603\n"
     ]
    }
   ],
   "source": [
    "version = 'uniter2'\n",
    "n_iter = 1000\n",
    "file_path = '../model-result/randomForest/model_'+str(version)\n",
    "\n",
    "all_models = ['uniter_36feats', 'uniter_50feats']\n",
    "all_vars = text_feature + [s + '_proba' for s in all_models]\n",
    "measure_tab, rf_random = main(train_data, test_data, all_vars, param_list=random_grid, file_path = file_path, random_state = 17, n_iter = n_iter)  \n",
    "print('test_unseen')\n",
    "print(max_vote(test_data, all_models))\n",
    "print(average_vote(test_data, all_models))\n",
    "\n",
    "print('dev_unseen')\n",
    "print(max_vote(dev_unseen_all, all_models))\n",
    "print(average_vote(dev_unseen_all, all_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n",
      "RandomForestClassifier(bootstrap=False, max_depth=2, max_features='sqrt',\n",
      "                       min_samples_leaf=2, min_samples_split=4, n_estimators=5)\n"
     ]
    }
   ],
   "source": [
    "version = 'uniter'\n",
    "n_iter = 1000\n",
    "file_path = '../model-result/randomForest/model_'+str(version)\n",
    "      \n",
    "measure_tab, rf_random = main(train_data, test_data, vars, param_list=random_grid, file_path = file_path, random_state = 17, n_iter = n_iter)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n",
      "RandomForestClassifier(max_depth=2, max_features='sqrt', min_samples_split=6,\n",
      "                       n_estimators=49)\n"
     ]
    }
   ],
   "source": [
    "vars = ['profanity', 'nationality', 'racism', 'gender', 'disability', 'pregnancy', 'religion' , 'hatespeech_prob', \n",
    "'detectron_VisualBert_cc_10pct_proba',\n",
    "       'detectron_VisualBert_cc_50pct_proba',\n",
    "       'detectron_VisualBert_coco_proba',\n",
    "       'detectron_VisualBert_coco_focalLoss_proba',\n",
    "       'detectron_VisualBert_cc_50pct_focalLoss_proba'] \n",
    "\n",
    "version = 'no_uniter'\n",
    "file_path = '../model-result/randomForest/model_'+str(version)\n",
    "measure_tab, rf_random = main(train_data, test_data, vars, param_list=random_grid, file_path = file_path, random_state = 17, n_iter = n_iter)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgst(train_data, test_data, vars, file_path,random_state):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    \n",
    "    X_train = train_data.loc[:, vars]\n",
    "    y_train = train_data['label']\n",
    "\n",
    "    X_test = test_data.loc[:, vars]\n",
    "    y_test = test_data['label']\n",
    "    # build model\n",
    "    model = LogisticRegression(penalty='none', fit_intercept=False, random_state=random_state, class_weight='balanced').fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    y_pred_label = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred_label)\n",
    "    f1 = metrics.f1_score(y_test, y_pred_label)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    measure_tab = pd.DataFrame([{'Acc': acc,'F1': f1, 'auc': auc}])\n",
    "    measure_tab.to_csv(file_path +'/lgst'+str(random_state)+'.csv',index = False)\n",
    "    #print(y_fitted)\n",
    "    return measure_tab, model\n",
    "\n",
    "\n",
    "version = 'more_vars'\n",
    "file_path = '../model-result/lgst/model'+str(version)\n",
    "measure_tab, model = lgst(train_data, test_data, extra_var, file_path = file_path, random_state = 17)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['profanity', 'nationality', 'racism', 'gender', 'disability', 'pregnancy', 'religion', 'hatespeech_prob', 'detectron_VisualBert_cc_10pct_proba',\n",
    "       'detectron_VisualBert_cc_50pct_proba',\n",
    "       'detectron_VisualBert_coco_proba',\n",
    "       'detectron_VisualBert_coco_focalLoss_proba'] \n",
    "\n",
    "version = 'no_uniter'\n",
    "file_path = '../model-result/lgst/model_'+str(version)\n",
    "measure_tab, model = lgst(train_data, test_data, vars, file_path = file_path, random_state = 17)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-hoc\n",
    "Add memes will be treated as hateful if they trigger at least 2 sensitive works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_unseen_tag.loc[:, 'profanity':'religion'] = np.where(dev_unseen_tag.loc[:, 'profanity':'religion'].isnull(), 0, 1)\n",
    "#print(dev_unseen_tag)\n",
    "#dev_unseen_tag.loc[:, 'profanity':'religion'].(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[386 516 743]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>proba</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>87549</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>71420</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>31957</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     proba  label\n",
       "386  87549 -0.002486      1\n",
       "516  71420 -0.000040      1\n",
       "743  31957 -0.000034      1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(dev_unseen_tag.loc[:, 'profanity':'religion'].sum(axis = 1)>2)\n",
    "print(np.where(test_unseen_tag.loc[:, 'profanity':'religion'].sum(axis = 1)>2)[0])\n",
    "\n",
    "tmp = pd.read_csv('../model-result/uniter_36feats_test_unseen.csv')\n",
    "tmp.loc[np.where(test_unseen_tag.loc[:, 'profanity':'religion'].sum(axis = 1)>2)[0],:]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9aa4d9e6388e68c5f332c40a811aee1d22fa4c31e580e452f893e4daae6764b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cs7643-a2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
